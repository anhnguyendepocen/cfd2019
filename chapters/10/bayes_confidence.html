<!doctype html>
<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>9.3 Back to Havana - Coding for Data - 2019 edition</title>
<meta name="description" content="In the problem for the education minister we had a sample of fast-track-marked exams from2019, and we found that the mean mark was 58.74.  We wondered what we could sayabout the eventual mean of the marks for all 8000 or so students.">



<meta property="og:type" content="article">
<meta property="og:locale" content="en_UK">
<meta property="og:site_name" content="Coding for Data - 2019 edition">
<meta property="og:title" content="9.3 Back to Havana">
<meta property="og:url" content="https://matthew-brett.github.io/cfd2019/chapters/10/bayes_confidence">


  <meta property="og:description" content="In the problem for the education minister we had a sample of fast-track-marked exams from2019, and we found that the mean mark was 58.74.  We wondered what we could sayabout the eventual mean of the marks for all 8000 or so students.">







  <meta property="article:published_time" content="2020-05-23T16:16:24+01:00">





  

  


<link rel="canonical" href="https://matthew-brett.github.io/cfd2019/chapters/10/bayes_confidence">







  <script type="application/ld+json">
    {
      "@context": "http://schema.org",
      "@type": "Person",
      "name": "Matthew Brett",
      "url": "https://matthew-brett.github.io/cfd2019",
      "sameAs": null
    }
  </script>







<!-- end _includes/seo.html -->


<link href="/cfd2019/feed.xml" type="application/atom+xml" rel="alternate" title="Coding for Data - 2019 edition Feed">

<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/cfd2019/assets/css/main.css">

<!--[if lte IE 9]>
  <style>
    /* old IE unsupported flexbox fixes */
    .greedy-nav .site-title {
      padding-right: 3em;
    }
    .greedy-nav button {
      position: absolute;
      top: 0;
      right: 0;
      height: 100%;
    }
  </style>
<![endif]-->


    <!-- start custom head snippets -->


<!-- end custom head snippets -->

    <link rel="stylesheet" href="/cfd2019/assets/css/notebook-markdown.css">
    <link rel="stylesheet" href="/cfd2019/assets/css/custom.css">
    <link rel="shortcut icon" type="image/png" href="/cfd2019/favicon.png">
    <script src="https://cdn.jsdelivr.net/npm/clipboard@1/dist/clipboard.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/anchor-js/4.1.1/anchor.min.js"></script>
  </head>

  <body class="layout--textbook">

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->

    

    <div class="initial-content">
      



<div id="main" class="textbook" role="main">
  <div id="textbook_wrapper">
    
  <div class="sidebar sticky textbook">
  
  
    <img src="/cfd2019/images/dsfe_logo.png" class="textbook_logo" />
    

    
      

<nav class="nav__list">
  
  <input id="ac-toc" name="accordion-toc" type="checkbox" />
  <label for="ac-toc">Toggle Menu</label>
  <ul class="nav__items">
    
      <li>
        
          
          

          <a href="/cfd2019/"><span class="nav__sub-title">Home</span></a>
        

        
      </li>
    
      <li>
        
          
          

          <a href="/cfd2019/chapters/01/intro"><span class="nav__sub-title">1. Coding for data</span></a>
        

        
        <ul>
          
            
            

            
            

            

            <li><a href="/cfd2019/chapters/01/what-is-data-science" class="level_1">1.1 What is data science?</a></li>
          
            
            

            
            

            

            <li><a href="/cfd2019/chapters/01/why-data-science" class="level_1">1.2 Why data science?</a></li>
          
            
            

            
            

            

            <li><a href="/cfd2019/chapters/01/tools_techniques" class="level_1">1.3 Tools and techniques</a></li>
          
            
            

            
            

            

            <li><a href="/cfd2019/chapters/01/computational-tools" class="level_2">1.3.1 Computational tools</a></li>
          
            
            

            
            

            

            <li><a href="/cfd2019/chapters/01/statistical-techniques" class="level_2">1.3.2 Statistical techniques</a></li>
          
            
            

            
            

            

            <li><a href="/cfd2019/chapters/01/Plotting_the_Classics" class="level_1">1.4 Plotting the classics</a></li>
          
            
            

            
            

            

            <li><a href="/cfd2019/chapters/01/Literary_Characters" class="level_2">1.4.1 Literary characters</a></li>
          
            
            

            
            

            

            <li><a href="/cfd2019/chapters/01/Another_Kind_Of_Character" class="level_2">1.4.2 Another kind of character</a></li>
          
            
            

            
            

            

            <li><a href="/cfd2019/chapters/01/surviving_computers" class="level_1">1.5 Surviving the computer</a></li>
          
            
            

            
            

            

            <li><a href="/cfd2019/chapters/01/the_software" class="level_1">1.6 About the software</a></li>
          
            
            

            
            

            

            <li><a href="/cfd2019/chapters/01/using_jupyter" class="level_1">1.7 Using the Jupyter notebook</a></li>
          
            
            

            
            

            

            <li><a href="/cfd2019/chapters/01/more_on_jupyter" class="level_1">1.8 More on the Jupyter notebook</a></li>
          
        </ul>
        
      </li>
    
      <li>
        
          
          

          <a href="/cfd2019/chapters/02/to_code"><span class="nav__sub-title">2. Programming</span></a>
        

        
        <ul>
          
            
            

            
            

            

            <li><a href="/cfd2019/chapters/02/sampling_problem" class="level_1">2.1 A sampling problem</a></li>
          
            
            

            
            

            

            <li><a href="/cfd2019/chapters/02/three_girls" class="level_1">2.2 A simpler problem</a></li>
          
            
            

            
            

            

            <li><a href="/cfd2019/chapters/02/variables_intro" class="level_1">2.3 Introduction to variables</a></li>
          
            
            

            
            

            

            <li><a href="/cfd2019/chapters/02/functions" class="level_1">2.4 Introduction to functions</a></li>
          
            
            

            
            

            

            <li><a href="/cfd2019/chapters/02/first_pass_three_girls" class="level_1">2.5 A first pass</a></li>
          
            
            

            
            

            

            <li><a href="/cfd2019/chapters/02/Expressions" class="level_1">2.6 Expressions</a></li>
          
            
            

            
            

            

            <li><a href="/cfd2019/chapters/02/Calls" class="level_1">2.7 Call expressions</a></li>
          
            
            

            
            

            

            <li><a href="/cfd2019/chapters/02/sub_expressions" class="level_1">2.8 Sub-expressions</a></li>
          
            
            

            
            

            

            <li><a href="/cfd2019/chapters/02/Names" class="level_1">2.9 Names and variables</a></li>
          
        </ul>
        
      </li>
    
      <li>
        
          
          

          <a href="/cfd2019/chapters/03/data_types"><span class="nav__sub-title">3. Data types</span></a>
        

        
        <ul>
          
            
            

            
            

            

            <li><a href="/cfd2019/chapters/03/Numbers" class="level_1">3.1 Numbers</a></li>
          
            
            

            
            

            

            <li><a href="/cfd2019/chapters/03/Strings" class="level_1">3.2 Strings</a></li>
          
            
            

            
            

            

            <li><a href="/cfd2019/chapters/03/String_Methods" class="level_2">3.2.1 String methods</a></li>
          
            
            

            
            

            

            <li><a href="/cfd2019/chapters/03/Comparison" class="level_1">3.3 Comparison</a></li>
          
            
            

            
            

            

            <li><a href="/cfd2019/chapters/03/lists" class="level_1">3.4 Lists</a></li>
          
            
            

            
            

            

            <li><a href="/cfd2019/chapters/03/Arrays" class="level_1">3.5 Arrays</a></li>
          
            
            

            
            

            

            <li><a href="/cfd2019/chapters/03/Ranges" class="level_1">3.6 Ranges</a></li>
          
            
            

            
            

            

            <li><a href="/cfd2019/chapters/03/numpy_append" class="level_1">3.7 Append</a></li>
          
            
            

            
            

            

            <li><a href="/cfd2019/chapters/03/function_arguments" class="level_1">3.8 Function arguments</a></li>
          
            
            

            
            

            

            <li><a href="/cfd2019/chapters/03/leaping_ahead" class="level_1">3.9 Leaping ahead</a></li>
          
            
            

            
            

            

            <li><a href="/cfd2019/chapters/03/iteration" class="level_1">3.10 Iteration with For loops</a></li>
          
            
            

            
            

            

            <li><a href="/cfd2019/chapters/03/indentation" class="level_1">3.11 Indentation, indentation</a></li>
          
            
            

            
            

            

            <li><a href="/cfd2019/chapters/03/reply_supreme" class="level_1">3.12 Reply to the Supreme Court</a></li>
          
            
            

            
            

            

            <li><a href="/cfd2019/chapters/03/More_on_Arrays" class="level_1">3.13 More on arrays</a></li>
          
            
            

            
            

            

            <li><a href="/cfd2019/chapters/03/array_indexing" class="level_1">3.14 Selecting in arrays</a></li>
          
            
            

            
            

            

            <li><a href="/cfd2019/chapters/03/filling_arrays" class="level_1">3.15 Filling arrays</a></li>
          
        </ul>
        
      </li>
    
      <li>
        
          
          

          <a href="/cfd2019/chapters/04/data_frames"><span class="nav__sub-title">4. Data frames</span></a>
        

        
        <ul>
          
            
            

            
            

            

            <li><a href="/cfd2019/chapters/04/data_frame_intro" class="level_1">4.1 Introduction to data frames</a></li>
          
            
            

            
            

            

            <li><a href="/cfd2019/chapters/04/df_series_arrays" class="level_1">4.2 Data frames, Series and arrays</a></li>
          
        </ul>
        
      </li>
    
      <li>
        
          
          

          <a href="/cfd2019/chapters/05/permutation"><span class="nav__sub-title">5. Permutations</span></a>
        

        
        <ul>
          
            
            

            
            

            

            <li><a href="/cfd2019/chapters/05/population_permutation" class="level_1">5.1 Population and permutation</a></li>
          
            
            

            
            

            

            <li><a href="/cfd2019/chapters/05/brexit_ages" class="level_1">5.2 A permutation test</a></li>
          
            
            

            
            

            

            <li><a href="/cfd2019/chapters/05/permutation_idea" class="level_1">5.3 The permutation idea</a></li>
          
            
            

            
            

            

            <li><a href="/cfd2019/chapters/05/permutation_and_t_test" class="level_1">5.4 Permutation and the t-test</a></li>
          
            
            

            
            

            

            <li><a href="/cfd2019/chapters/05/testing_t" class="level_1">5.5 Testing validity of tests</a></li>
          
        </ul>
        
      </li>
    
      <li>
        
          
          

          <a href="/cfd2019/chapters/07/more_building_blocks"><span class="nav__sub-title">6. More building blocks</span></a>
        

        
        <ul>
          
            
            

            
            

            

            <li><a href="/cfd2019/chapters/07/introducing_functions" class="level_1">6.1 Introduction to functions</a></li>
          
            
            

            
            

            

            <li><a href="/cfd2019/chapters/07/none" class="level_1">6.2 On None</a></li>
          
            
            

            
            

            

            <li><a href="/cfd2019/chapters/07/functions" class="level_1">6.3 Functions in more detail</a></li>
          
            
            

            
            

            

            <li><a href="/cfd2019/chapters/07/functions_as_values" class="level_1">6.4 Functions as values</a></li>
          
            
            

            
            

            

            <li><a href="/cfd2019/chapters/07/conditional_statements" class="level_1">6.5 Conditional statements</a></li>
          
            
            

            
            

            

            <li><a href="/cfd2019/chapters/07/pandas_indexing" class="level_1">6.6 Indexing in Pandas</a></li>
          
            
            

            
            

            

            <li><a href="/cfd2019/chapters/07/noble_politics" class="level_1">6.7 Example: noble politics</a></li>
          
            
            

            
            

            

            <li><a href="/cfd2019/chapters/07/safe_pandas" class="level_1">6.8 Safe Pandas</a></li>
          
            
            

            
            

            

            <li><a href="/cfd2019/chapters/07/text_encoding" class="level_1">6.9 Text encoding</a></li>
          
            
            

            
            

            

            <li><a href="/cfd2019/chapters/07/numbers_and_strings" class="level_1">6.10 Numbers and strings</a></li>
          
        </ul>
        
      </li>
    
      <li>
        
          
          

          <a href="/cfd2019/chapters/08/mean"><span class="nav__sub-title">7. The mean and straight line relationships</span></a>
        

        
        <ul>
          
            
            

            
            

            

            <li><a href="/cfd2019/chapters/08/mean_meaning" class="level_1">7.1 The mean as a predictor</a></li>
          
            
            

            
            

            

            <li><a href="/cfd2019/chapters/08/where_and_argmin" class="level_1">7.2 Where and argmin</a></li>
          
            
            

            
            

            

            <li><a href="/cfd2019/chapters/08/mean_and_slopes" class="level_1">7.3 Mean and slopes</a></li>
          
            
            

            
            

            

            <li><a href="/cfd2019/chapters/08/optimization" class="level_1">7.4 Optimization</a></li>
          
            
            

            
            

            

            <li><a href="/cfd2019/chapters/08/finding_lines" class="level_1">7.5 Finding lines</a></li>
          
            
            

            
            

            

            <li><a href="/cfd2019/chapters/08/using_minimize" class="level_1">7.6 Using minimize</a></li>
          
            
            

            
            

            

            <li><a href="/cfd2019/chapters/08/inference_on_slopes" class="level_1">7.7 Believable slopes</a></li>
          
            
            

            
            

            

            <li><a href="/cfd2019/chapters/08/combining_boolean_arrays" class="level_1">7.8 Combining Booleans</a></li>
          
            
            

            
            

            

            <li><a href="/cfd2019/chapters/08/standard_scores" class="level_1">7.9 Standard scores</a></li>
          
            
            

            
            

            

            <li><a href="/cfd2019/chapters/08/Correlation" class="level_1">7.10 Correlation</a></li>
          
        </ul>
        
      </li>
    
      <li>
        
          
          

          <a href="/cfd2019/chapters/09/classification"><span class="nav__sub-title">8. Classification</span></a>
        

        
        <ul>
          
            
            

            
            

            

            <li><a href="/cfd2019/chapters/09/Nearest_Neighbors" class="level_1">8.2 Nearest neighbors</a></li>
          
            
            

            
            

            

            <li><a href="/cfd2019/chapters/09/Training_and_Testing" class="level_1">8.3 Training and testing</a></li>
          
            
            

            
            

            

            <li><a href="/cfd2019/chapters/09/Rows_of_Tables" class="level_1">8.4 Rows of tables</a></li>
          
            
            

            
            

            

            <li><a href="/cfd2019/chapters/09/Implementing_the_Classifier" class="level_1">8.5 Implementing the classifier</a></li>
          
            
            

            
            

            

            <li><a href="/cfd2019/chapters/09/Accuracy_of_the_Classifier" class="level_1">8.6 Accuracy of the classifier</a></li>
          
        </ul>
        
      </li>
    
      <li>
        
          
          

          <a href="/cfd2019/chapters/10/confidence"><span class="nav__sub-title">9. Confidence</span></a>
        

        
        <ul>
          
            
            

            
            

            

            <li><a href="/cfd2019/chapters/10/havana_math" class="level_1">9.1 The education minister</a></li>
          
            
            

            
            

            

            <li><a href="/cfd2019/chapters/10/random_choice" class="level_1">9.2 Random choice</a></li>
          
            
            

            
            

            

            <li><a href="/cfd2019/chapters/10/first_bayes" class="level_1">9.2 Reverse probability</a></li>
          
            
            

            
            

            

            <li><a href="/cfd2019/chapters/10/bayes_bars" class="level_1">9.3 Bayes bars</a></li>
          
            
            

            
            

            

            <li><a href="/cfd2019/chapters/10/second_bayes" class="level_1">9.4 Confidence in bars</a></li>
          
        </ul>
        
      </li>
    
      <li>
        
          
          

          <a href="/cfd2019/chapters/end/end_of_beginning"><span class="nav__sub-title">10. The end of the beginning</span></a>
        

        
      </li>
    
      <li>
        
          
          

          <a href="/cfd2019/chapters/exercises/exercises"><span class="nav__sub-title">Exercises</span></a>
        

        
        <ul>
          
        </ul>
        
      </li>
    
      <li>
        
          
          

          <a href="/cfd2019/chapters/extra/extra"><span class="nav__sub-title">Extra pages</span></a>
        

        
        <ul>
          
            
            

            
            

            

            <li><a href="/cfd2019/chapters/extra/more_on_lists" class="level_1">More on lists</a></li>
          
            
            

            
            

            

            <li><a href="/cfd2019/chapters/extra/monty_hall_lists" class="level_1">Monty Hall with lists</a></li>
          
            
            

            
            

            

            <li><a href="/cfd2019/chapters/extra/data8_functions" class="level_1">Berkeley introduction to functions</a></li>
          
            
            

            
            

            

            <li><a href="/cfd2019/chapters/extra/mean_deviations" class="level_1">Deviations around the mean</a></li>
          
            
            

            
            

            

            <li><a href="/cfd2019/chapters/extra/mean_sq_deviations" class="level_1">Squared deviations around the mean</a></li>
          
            
            

            
            

            

            <li><a href="/cfd2019/chapters/extra/slope_deviations" class="level_1">Finding the best slope</a></li>
          
        </ul>
        
      </li>
    
  </ul>
</nav>

    

  
  </div>


    <article class="page textbook" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="headline" content="9.3 Back to Havana">
      <meta itemprop="description" content="In the problem for the education minister we had a sample of fast-track-marked exams from2019, and we found that the mean mark was 58.74.  We wondered what we could sayabout the eventual mean of the marks for all 8000 or so students.">
      <meta itemprop="datePublished" content="May 23, 2020">
      

      <div class="page__inner-wrap">
        
          <header>
            <h1 id="page-title" class="page__title" itemprop="headline">9.3 Back to Havana
</h1>
          </header>
        

        <section class="page__content" itemprop="text">
          
            

<!-- TOC will only show up if it has at least one item -->


  <aside class="sidebar__right">
    <nav class="toc">
      <header><h4 class="nav__title"><i class="fas fa-list-ul"></i>   On this page</h4></header>
      <ul class="toc__menu">
  <li><a href="#reversing-the-probabilities">Reversing the probabilities</a></li>
  <li><a href="#a-short-cut-for-the-calculations">A short cut for the calculations</a></li>
  <li><a href="#estimating-the-sampling-distribution">Estimating the sampling distribution</a></li>
</ul>
    </nav>
  </aside>


          
          <!-- INTERACT LINKS -->

    
    
    <a class="notebook-link" href="https://matthew-brett.github.io/cfd2019/ipynb/10/bayes_confidence.ipynb">Download notebook</a>
    <a class="interact-button" href="https://mybinder.org/v2/gh/matthew-brett/cfd2019/master?filepath=ipynb%2F10%2Fbayes_confidence.ipynb">Interact</a>


          <p>In the <a href="/cfd2019/chapters/10/havana_math">problem for the education minister</a> we had a sample of fast-track-marked exams from
2019, and we found that the mean mark was 58.74.  We wondered what we could say
about the eventual mean of the marks for all 8000 or so students.</p>

<p>After a bit of development, we found, in the <a href="/cfd2019/chapters/10/bayes_bars">reverse probability with bars</a> page, that we could use some
probability calculations to draw conclusions about the state of the world, from
some result.  In that page, we calculated the probability of the state of the
world (a box we have been given) from a result (drawing a red ball).</p>

<p>Now we want to draw a conclusion about the state of the world (the eventual
mean of all the 2019 exams) from a result (the mean of the fast-marked sample
of 50 2019 exams).</p>

<p>We will call the 50 fast-track-marked exams the <em>sample</em>.  When the 2019
marking is finished, we will have around 8000 marks.  We will call this the
<em>population</em>.  We want to draw conclusions about the <em>population</em> from the
<em>sample</em>.   In particular we want to draw conclusions about the population mean
from the sample mean.</p>

<p>Let us start with the following problem:</p>

<p><strong>Problem 1</strong>: What is the probability that we will observe a <em>sample</em> mean of
<em>around</em> 58.74, given that the <em>population</em> mean is 62.25?</p>

<p>Referring back to our box and ball problem, this probability is the equivalent
of the probability of getting a red ball from a given box.  Given a state of
the world (the population mean) what the is the probability of the result (the
sample mean).  Once we have probabilities like these, we will be able to use
the logic you have already seen to get the <em>reverse</em> probability - how likely
was any particular state of the world (population mean), given the result (the
sample mean).</p>

<p>Returning to our mathematics exam problem: how will we calculate the
probability of a sample mean of around 58.74, given a population mean of 62.25?</p>

<p>As usual, this is a problem of <em>sampling</em>.  If the mean of the population is
62.25, and we draw a sample of 50 marks, then the mean of the sample will be
vary somewhat depending on the sample. That is, the <em>sample mean</em> will be 62.25
plus or minus a bit.  As usual, we need to quantify what we mean by “a bit”.</p>

<p>For example, remember the population of 2018 marks, that do have a mean of
around 62.25.</p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="c1"># Clean up display of small numbers.
</span><span class="n">np</span><span class="o">.</span><span class="n">set_printoptions</span><span class="p">(</span><span class="n">precision</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">suppress</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s">'fivethirtyeight'</span><span class="p">)</span>
</code></pre></div></div>

<p>You can download the 2018 marks from <a href="/cfd2019/data/havana_math_2018.csv">havana_math_2018.csv</a>.</p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">havana_2018</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">'havana_math_2018.csv'</span><span class="p">)</span>
<span class="c1"># Drop missing marks.
</span><span class="n">marks_2018</span> <span class="o">=</span> <span class="n">havana_2018</span><span class="p">[</span><span class="s">'mark'</span><span class="p">]</span><span class="o">.</span><span class="n">dropna</span><span class="p">()</span>
<span class="n">mean_2018</span> <span class="o">=</span> <span class="n">marks_2018</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="n">mean_2018</span>
</code></pre></div></div>

<div class="output_data_text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>65.25609088420477
</code></pre></div></div>

<p>This was our sample of 50 marks from the 2019 examinations.  It has a mean of
58.74.</p>

<p>You can download the sample file from <a href="/cfd2019/data/havana_math_2019_sample.csv">havana_math_2019_sample.csv</a>.</p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">havana_2019_sample</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">'havana_math_2019_sample.csv'</span><span class="p">)</span>
<span class="n">observed_sample_mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">havana_2019_sample</span><span class="p">[</span><span class="s">'mark'</span><span class="p">])</span>
<span class="n">observed_sample_mean</span>
</code></pre></div></div>

<div class="output_data_text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>58.74
</code></pre></div></div>

<p>To get the <em>sampling distribution</em> of the mean of a sample of 50, we would have
to calculate the mean for every possible sample of 50 values from the 7300 or
so marks.  As usual, we make do with an <em>estimate</em> of the sampling distribution
by taking many thousands of samples.</p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Take 100000 samples, calculate their means.
</span><span class="n">n_samples</span> <span class="o">=</span> <span class="mi">100000</span>
<span class="n">sample_means</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_samples</span><span class="p">):</span>
    <span class="n">sample</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">marks_2018</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
    <span class="n">sample_means</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">sample</span><span class="p">)</span>
<span class="n">sample_means</span><span class="p">[:</span><span class="mi">5</span><span class="p">]</span>
</code></pre></div></div>

<div class="output_data_text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>array([71.36, 62.3 , 71.7 , 64.46, 67.36])
</code></pre></div></div>

<p>As expected, the mean of the <em>sampling distribution</em> is very close to the mean
of the population:</p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">sample_means</span><span class="p">)</span>
</code></pre></div></div>

<div class="output_data_text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>65.25597699999999
</code></pre></div></div>

<p>In what follows, we will ignore the small difference between the mean of the
sampling distribution, and the mean of the population.</p>

<p>The next cell has a histogram of the sampling distribution.  Notice that we
have asked <code class="highlighter-rouge">plt.hist</code> to break the histogram into bins with <em>edges</em>
<code class="highlighter-rouge">np.arange(50, 80, 0.5)</code>.  This means that each bin covers a range of 0.5 units
— so the first bin in the histogram gives the counts of all sample means that
were between 50 and 50.5 (excluding 50.5), the second bin covers 50.5 up to
(not including) 51, and so on.</p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">bin_edges</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="mi">80</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">sample_means</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="n">bin_edges</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'Mean mark for sample of 50'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Estimated sampling distribution for mean of 50 marks'</span><span class="p">);</span>
</code></pre></div></div>

<p><img src="../../images/chapters/10/bayes_confidence_12_0.png" alt="png" /></p>

<p>Notice that the distribution has a very slightly longer left tail.</p>

<p>We can use <code class="highlighter-rouge">plt.hist</code> to give us the counts for each of these bins, by storing
the values that <code class="highlighter-rouge">plt.hist</code> returns, like this:</p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Store the values that plt.hist returns.
</span><span class="n">hist_vals</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">sample_means</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="n">bin_edges</span><span class="p">)</span>
<span class="c1"># Counts per bin is the first returned value.
</span><span class="n">counts</span> <span class="o">=</span> <span class="n">hist_vals</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">counts</span>
</code></pre></div></div>

<div class="output_data_text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>array([   2.,    3.,   12.,   10.,   16.,   21.,   54.,   56.,   80.,
        132.,  152.,  222.,  327.,  406.,  585.,  731.,  956., 1167.,
       1428., 1889., 2155., 2634., 3027., 3236., 3806., 4107., 4481.,
       4939., 4935., 5287., 5253., 5188., 5174., 4946., 4639., 4291.,
       3875., 3523., 3188., 2672., 2283., 1817., 1587., 1201.,  961.,
        722.,  541.,  421.,  273.,  189.,  131.,  107.,   61.,   41.,
         23.,   10.,   11.,    6.,    3.])
</code></pre></div></div>

<p><img src="../../images/chapters/10/bayes_confidence_14_1.png" alt="png" /></p>

<p>As we saw in the page on <a href="/cfd2019/chapters/08/using_minimize#unpacking">using minimize</a>, we can get this value a little more
neatly by <em>unpacking</em> the return values from <code class="highlighter-rouge">plt.hist</code>, like this:</p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Store the values that plt.hist returns.
# We will only use the first of these.
</span><span class="n">counts</span><span class="p">,</span> <span class="n">edges</span><span class="p">,</span> <span class="n">patches</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">sample_means</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="n">bin_edges</span><span class="p">)</span>
<span class="c1"># Counts per bin (again):
</span><span class="n">counts</span>
</code></pre></div></div>

<div class="output_data_text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>array([   2.,    3.,   12.,   10.,   16.,   21.,   54.,   56.,   80.,
        132.,  152.,  222.,  327.,  406.,  585.,  731.,  956., 1167.,
       1428., 1889., 2155., 2634., 3027., 3236., 3806., 4107., 4481.,
       4939., 4935., 5287., 5253., 5188., 5174., 4946., 4639., 4291.,
       3875., 3523., 3188., 2672., 2283., 1817., 1587., 1201.,  961.,
        722.,  541.,  421.,  273.,  189.,  131.,  107.,   61.,   41.,
         23.,   10.,   11.,    6.,    3.])
</code></pre></div></div>

<p><img src="../../images/chapters/10/bayes_confidence_16_1.png" alt="png" /></p>

<p>If we show the counts as a bar graph, it is the same as the histogram, because
it is using the same values.  We use the bin centers instead of the bin edges
for the x axis, as the histogram routine does, internally.</p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Get bin centers by dropping the right hand edge, add half bin width.
</span><span class="n">bin_centers</span> <span class="o">=</span> <span class="n">bin_edges</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="mf">0.25</span>
<span class="n">bin_centers</span>
</code></pre></div></div>

<div class="output_data_text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>array([50.25, 50.75, 51.25, 51.75, 52.25, 52.75, 53.25, 53.75, 54.25,
       54.75, 55.25, 55.75, 56.25, 56.75, 57.25, 57.75, 58.25, 58.75,
       59.25, 59.75, 60.25, 60.75, 61.25, 61.75, 62.25, 62.75, 63.25,
       63.75, 64.25, 64.75, 65.25, 65.75, 66.25, 66.75, 67.25, 67.75,
       68.25, 68.75, 69.25, 69.75, 70.25, 70.75, 71.25, 71.75, 72.25,
       72.75, 73.25, 73.75, 74.25, 74.75, 75.25, 75.75, 76.25, 76.75,
       77.25, 77.75, 78.25, 78.75, 79.25])
</code></pre></div></div>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">bin_centers</span><span class="p">,</span> <span class="n">counts</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Replicate sampling distribution using counts'</span><span class="p">);</span>
</code></pre></div></div>

<p><img src="../../images/chapters/10/bayes_confidence_19_0.png" alt="png" /></p>

<p>For each bin, <code class="highlighter-rouge">counts</code> gives the count of the 10000 samples we took had a mean
between the bin edges.  For example, the center of the bin at index 20 is:</p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">bin_centers</span><span class="p">[</span><span class="mi">20</span><span class="p">]</span>
</code></pre></div></div>

<div class="output_data_text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>60.25
</code></pre></div></div>

<p>This the bin counting all the sample mean values between 60 and 60.5.  It has a
lower edge of 60, and an upper edge of 60.5.</p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="s">'Lower edge:'</span><span class="p">,</span> <span class="n">bin_edges</span><span class="p">[</span><span class="mi">20</span><span class="p">])</span>
<span class="k">print</span><span class="p">(</span><span class="s">'Upper edge (not included):'</span><span class="p">,</span> <span class="n">bin_edges</span><span class="p">[</span><span class="mi">21</span><span class="p">])</span>
</code></pre></div></div>

<div class="output_stream highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Lower edge: 60.0
Upper edge (not included): 60.5

</code></pre></div></div>

<p>The count in that bin is:</p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">counts</span><span class="p">[</span><span class="mi">20</span><span class="p">]</span>
</code></pre></div></div>

<div class="output_data_text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>2155.0
</code></pre></div></div>

<p>This is the count of the 100000 sample means from our estimated sampling
distribution, that were from 60 up to, but not including, 60.5.</p>

<p>Dividing the counts by the number of samples, we get the proportion of samples
that fall in this range:</p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">proportions</span> <span class="o">=</span> <span class="n">counts</span> <span class="o">/</span> <span class="n">n_samples</span>
<span class="n">proportions</span>
</code></pre></div></div>

<div class="output_data_text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>array([0.    , 0.    , 0.0001, 0.0001, 0.0002, 0.0002, 0.0005, 0.0006,
       0.0008, 0.0013, 0.0015, 0.0022, 0.0033, 0.0041, 0.0059, 0.0073,
       0.0096, 0.0117, 0.0143, 0.0189, 0.0215, 0.0263, 0.0303, 0.0324,
       0.0381, 0.0411, 0.0448, 0.0494, 0.0493, 0.0529, 0.0525, 0.0519,
       0.0517, 0.0495, 0.0464, 0.0429, 0.0387, 0.0352, 0.0319, 0.0267,
       0.0228, 0.0182, 0.0159, 0.012 , 0.0096, 0.0072, 0.0054, 0.0042,
       0.0027, 0.0019, 0.0013, 0.0011, 0.0006, 0.0004, 0.0002, 0.0001,
       0.0001, 0.0001, 0.    ])
</code></pre></div></div>

<p>For example, here is the proportion of sample means that fell between 60 and
60.5:</p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">proportions</span><span class="p">[</span><span class="mi">20</span><span class="p">]</span>
</code></pre></div></div>

<div class="output_data_text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>0.02155
</code></pre></div></div>

<p>In other words, given this estimated sampling distribution, for this world with
a mean of 62.25, the probability of any one sample mean being between 60 and
60.5 is:</p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">proportions</span><span class="p">[</span><span class="mi">20</span><span class="p">]</span>
</code></pre></div></div>

<div class="output_data_text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>0.02155
</code></pre></div></div>

<p>The bar graph of the proportions is the same as the histogram, but with the y
values divided by the number of samples (100000):</p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">bin_centers</span><span class="p">,</span> <span class="n">proportions</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Sampling distribution using proportions'</span><span class="p">);</span>
</code></pre></div></div>

<p><img src="../../images/chapters/10/bayes_confidence_33_0.png" alt="png" /></p>

<p>Now we are in a position to answer something approaching our intermediate
question:</p>

<p><strong>Problem 1</strong>: What is the probability that we will observe a <em>sample</em> mean of
<em>around</em> 58.74, given that the <em>population</em> mean is 62.25.</p>

<p>One difficulty with this question is that we do not know what the sampling
distribution would be for this hypothetical 2019 full set of marks, where the
population mean is 62.25.  For the moment, we will assume that the sampling
distribution is <em>exactly the same as it was in 2018</em>; this is the sampling
distribution we have already been using.</p>

<p>We find the bin corresponding to the sample mean of 58.74; this is the bin with
center 58.75, with edges 58.5 and 59.0.   It turns out this is the bin at index
17.</p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">bin_58p75</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">bin_centers</span> <span class="o">==</span> <span class="mf">58.75</span><span class="p">)</span>
<span class="n">bin_58p75</span>
</code></pre></div></div>

<div class="output_data_text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(array([17]),)
</code></pre></div></div>

<p>Look back at <a href="/cfd2019/chapters/08/where_and_argmin">where and argmin</a> for the trick here of using <code class="highlighter-rouge">np.where</code> to find
the index.</p>

<p>We get the proportion at that index, to give the probability that we will see a
sample mean between 58.5 and 59:</p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">p_around_58p75</span> <span class="o">=</span> <span class="n">proportions</span><span class="p">[</span><span class="n">bin_58p75</span><span class="p">]</span>
<span class="n">p_around_58p75</span>
</code></pre></div></div>

<div class="output_data_text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>array([0.0117])
</code></pre></div></div>

<p>We highlight this proportion in red:</p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">bin_centers</span><span class="p">,</span> <span class="n">proportions</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="mf">58.75</span><span class="p">,</span> <span class="n">p_around_58p75</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'red'</span><span class="p">)</span>
<span class="c1"># Store the x and y axis limits for later
</span><span class="n">xy_lims</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Proportion for the 58.75 bin'</span><span class="p">);</span>
</code></pre></div></div>

<p><img src="../../images/chapters/10/bayes_confidence_39_0.png" alt="png" /></p>

<p>This is the probability of something close to our observed sample mean (58.74)
given an eventual population mean of 62.25, and our assumed sampling
distribution.  The probability we have just found corresponds to the population
mean.  We start a new graph were we record the probability at its corresponding
population mean:</p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="mf">62.25</span><span class="p">,</span> <span class="n">p_around_58p75</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'red'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="n">xy_lims</span><span class="p">)</span>  <span class="c1"># Use the axis limits from the previous plot.
</span><span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Proportion against corresponding population mean'</span><span class="p">);</span>
</code></pre></div></div>

<p><img src="../../images/chapters/10/bayes_confidence_41_0.png" alt="png" /></p>

<p>Let us try a slightly more difficult intermediate problem:</p>

<p><strong>Problem 2</strong>: What is the probability that we will observe a <em>sample</em> mean of
<em>around</em> 58.74, given that the <em>population</em> mean is 61.75?</p>

<p>Notice the new hypothetical population mean that is the population mean from
2018, minus 0.5.</p>

<p>For problem 1, we assumed the sampling distribution was the same as it was in
2018, when the mean was, in fact, around 62.25.  Now we need the sampling
distribution for the case where the mean is 0.5 less, at 61.75.</p>

<p>We will assume that the <em>shape</em> of this hypothetical sampling distribution does
not change from the one we have used from 2018, but the <em>center</em> does change,
from 62.25 to 62.75. In other words, our assumed sampling distribution shifts
0.5 to the left on the x axis:</p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">bin_centers</span><span class="p">,</span> <span class="n">counts</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'blue'</span><span class="p">,</span>
   <span class="n">label</span><span class="o">=</span><span class="s">'Original'</span><span class="p">)</span>
<span class="c1"># Shift x values 0.5 (one bin) to the left.
</span><span class="n">bin_centers_1</span> <span class="o">=</span> <span class="n">bin_centers</span> <span class="o">-</span> <span class="mf">0.5</span>
<span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">bin_centers_1</span><span class="p">,</span> <span class="n">counts</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'red'</span><span class="p">,</span>
  <span class="n">label</span><span class="o">=</span><span class="s">'Shifted'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Original, shifted 2018 sampling distribution'</span><span class="p">);</span>
</code></pre></div></div>

<p><img src="../../images/chapters/10/bayes_confidence_43_0.png" alt="png" /></p>

<p>With the shifted sampling distribution, we just follow the same recipe as we
did for the population mean of 62.25.</p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">bin_58p75_1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">bin_centers_1</span> <span class="o">==</span> <span class="mf">58.75</span><span class="p">)</span>
<span class="n">bin_58p75_1</span>
</code></pre></div></div>

<div class="output_data_text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(array([18]),)
</code></pre></div></div>

<p>This is the bin at index 18 instead of the bin at index 17, that we found last
time.  We have shifted the distribution one bin width to the left, so our
corresponding bin in the sampling distribution is one bin to the right.</p>

<p>The proportion we want is:</p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">p_around_58p75_1</span> <span class="o">=</span> <span class="n">proportions</span><span class="p">[</span><span class="n">bin_58p75_1</span><span class="p">]</span>
<span class="n">p_around_58p75_1</span>
</code></pre></div></div>

<div class="output_data_text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>array([0.0143])
</code></pre></div></div>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">bin_centers_1</span><span class="p">,</span> <span class="n">proportions</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="mf">58.75</span><span class="p">,</span> <span class="n">p_around_58p75_1</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'red'</span><span class="p">)</span>
<span class="c1"># Store the x and y axis limits for later
</span><span class="n">xy_lims</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Proportion for 58.75 bin, pop mean 61.75'</span><span class="p">);</span>
</code></pre></div></div>

<p><img src="../../images/chapters/10/bayes_confidence_48_0.png" alt="png" /></p>

<p>This is the probability of something close to our observed sample mean (58.74)
given an eventual population mean of 61.75, and our assumed sampling
distribution. The probability corresponds to the eventual population mean, so
we add the probability value to the plot at this population mean:</p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># The previous p we found for 62.25
</span><span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="mf">62.25</span><span class="p">,</span> <span class="n">p_around_58p75</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'red'</span><span class="p">)</span>
<span class="c1"># The new p we found for 61.75
</span><span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="mf">61.75</span><span class="p">,</span> <span class="n">p_around_58p75_1</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'red'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="n">xy_lims</span><span class="p">)</span>  <span class="c1"># Use the axis limits from the previous plot.
</span><span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Proportion against corresponding population mean'</span><span class="p">);</span>
</code></pre></div></div>

<p><img src="../../images/chapters/10/bayes_confidence_50_0.png" alt="png" /></p>

<p>You can probably see how this is going to pan out now, but let us do the 61.25
bin for practice.</p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Original distribution.
</span><span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">bin_centers</span><span class="p">,</span> <span class="n">counts</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'blue'</span><span class="p">,</span>
   <span class="n">label</span><span class="o">=</span><span class="s">'Original'</span><span class="p">)</span>
<span class="c1"># Shift x values by 1 (two bins) to the left.
</span><span class="n">bin_centers_2</span> <span class="o">=</span> <span class="n">bin_centers</span> <span class="o">-</span> <span class="mi">1</span>
<span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">bin_centers_2</span><span class="p">,</span> <span class="n">counts</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'red'</span><span class="p">,</span>
  <span class="n">label</span><span class="o">=</span><span class="s">'Shifted by 2 bins'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Original, shifted-by-2 sampling distribution'</span><span class="p">);</span>
</code></pre></div></div>

<p><img src="../../images/chapters/10/bayes_confidence_52_0.png" alt="png" /></p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># We are now looking at the next bin to the right in the distribution.
</span><span class="n">bin_58p75_2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">bin_centers_2</span> <span class="o">==</span> <span class="mf">58.75</span><span class="p">)</span>
<span class="n">bin_58p75_2</span>
</code></pre></div></div>

<div class="output_data_text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(array([19]),)
</code></pre></div></div>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">p_around_58p75_2</span> <span class="o">=</span> <span class="n">proportions</span><span class="p">[</span><span class="n">bin_58p75_2</span><span class="p">]</span>
<span class="n">p_around_58p75_2</span>
</code></pre></div></div>

<div class="output_data_text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>array([0.0189])
</code></pre></div></div>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">bin_centers_2</span><span class="p">,</span> <span class="n">proportions</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="mf">58.75</span><span class="p">,</span> <span class="n">p_around_58p75_2</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'red'</span><span class="p">)</span>
<span class="c1"># Store the x and y axis limits for later
</span><span class="n">xy_lims</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Proportion for 58.75 bin, pop mean 61.75'</span><span class="p">);</span>
</code></pre></div></div>

<p><img src="../../images/chapters/10/bayes_confidence_55_0.png" alt="png" /></p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># The p we found for 62.25
</span><span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="mf">62.25</span><span class="p">,</span> <span class="n">p_around_58p75</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'red'</span><span class="p">)</span>
<span class="c1"># The p we found for 61.75
</span><span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="mf">61.75</span><span class="p">,</span> <span class="n">p_around_58p75_1</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'red'</span><span class="p">)</span>
<span class="c1"># The p we found for 61.25
</span><span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="mf">61.25</span><span class="p">,</span> <span class="n">p_around_58p75_2</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'red'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="n">xy_lims</span><span class="p">)</span>  <span class="c1"># Use the axis limits from the previous plot.
</span><span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Proportion against corresponding population mean'</span><span class="p">);</span>
</code></pre></div></div>

<p><img src="../../images/chapters/10/bayes_confidence_56_0.png" alt="png" /></p>

<p>We can repeat this procedure for every population mean.  For every population
mean, we shift the sampling distribution, and get the corresponding probability
of getting something in the bin of the observed sample mean of 58.74. This is
the bin between 58.5 and 59, centered on 58.75.</p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Bin centers at which to estimate probability.
</span><span class="n">population_means</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mf">42.75</span><span class="p">,</span> <span class="mf">67.75</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">)</span>
<span class="n">population_means</span>
</code></pre></div></div>

<div class="output_data_text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>array([42.75, 43.25, 43.75, 44.25, 44.75, 45.25, 45.75, 46.25, 46.75,
       47.25, 47.75, 48.25, 48.75, 49.25, 49.75, 50.25, 50.75, 51.25,
       51.75, 52.25, 52.75, 53.25, 53.75, 54.25, 54.75, 55.25, 55.75,
       56.25, 56.75, 57.25, 57.75, 58.25, 58.75, 59.25, 59.75, 60.25,
       60.75, 61.25, 61.75, 62.25, 62.75, 63.25, 63.75, 64.25, 64.75,
       65.25, 65.75, 66.25, 66.75, 67.25])
</code></pre></div></div>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">n_means</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">population_means</span><span class="p">)</span>
<span class="n">ps_for_mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_means</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_means</span><span class="p">):</span>
    <span class="c1"># Shift the x positions of the sampling distribution.
</span>    <span class="n">pop_mean</span> <span class="o">=</span> <span class="n">population_means</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="n">mean_diff</span> <span class="o">=</span> <span class="mf">62.25</span> <span class="o">-</span> <span class="n">pop_mean</span>
    <span class="n">new_bin_centers</span> <span class="o">=</span> <span class="n">bin_centers</span> <span class="o">-</span> <span class="n">mean_diff</span>
    <span class="c1"># Find the bin corresponding to the sample mean.
</span>    <span class="n">is_our_bin</span> <span class="o">=</span> <span class="n">new_bin_centers</span> <span class="o">==</span> <span class="mf">58.75</span>
    <span class="c1"># We might have gone too far, so there is no corresponding bin.
</span>    <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">count_nonzero</span><span class="p">(</span><span class="n">is_our_bin</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">continue</span>
    <span class="c1"># Store the probability for this population mean.
</span>    <span class="n">ps_for_mean</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">proportions</span><span class="p">[</span><span class="n">is_our_bin</span><span class="p">]</span>
<span class="n">ps_for_mean</span>
</code></pre></div></div>

<div class="output_data_text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>array([0.0001, 0.0001, 0.0002, 0.0004, 0.0006, 0.0011, 0.0013, 0.0019,
       0.0027, 0.0042, 0.0054, 0.0072, 0.0096, 0.012 , 0.0159, 0.0182,
       0.0228, 0.0267, 0.0319, 0.0352, 0.0387, 0.0429, 0.0464, 0.0495,
       0.0517, 0.0519, 0.0525, 0.0529, 0.0493, 0.0494, 0.0448, 0.0411,
       0.0381, 0.0324, 0.0303, 0.0263, 0.0215, 0.0189, 0.0143, 0.0117,
       0.0096, 0.0073, 0.0059, 0.0041, 0.0033, 0.0022, 0.0015, 0.0013,
       0.0008, 0.0006])
</code></pre></div></div>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">population_means</span><span class="p">,</span> <span class="n">ps_for_mean</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'red'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">'P values for sample mean around 58.75 given population means'</span><span class="p">);</span>
</code></pre></div></div>

<p><img src="../../images/chapters/10/bayes_confidence_60_0.png" alt="png" /></p>

<p>For reasons that might be clear from the calculations above, this probability
distribution is our original assumed sampling distribution, but:</p>

<ul>
  <li>Reversed so the right tail has become the left tail, and vice versa, and</li>
  <li>Shifted so that the mean of the distribution sits over the observed sample
mean.</li>
</ul>

<p>We can put both distributions on the plot to show this more clearly:</p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">population_means</span><span class="p">,</span> <span class="n">ps_for_mean</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'red'</span><span class="p">,</span>
   <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
   <span class="n">label</span><span class="o">=</span><span class="s">'Probability of sample mean'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">bin_centers</span><span class="p">,</span> <span class="n">proportions</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'blue'</span><span class="p">,</span>
    <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
    <span class="n">label</span><span class="o">=</span><span class="s">'Original proportions'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s">'lower left'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">'P values for sample mean and sampling distribution'</span><span class="p">);</span>
</code></pre></div></div>

<p><img src="../../images/chapters/10/bayes_confidence_62_0.png" alt="png" /></p>

<h2 id="reversing-the-probabilities">Reversing the probabilities</h2>

<p>Remember that each of the p values in shown on the y axis in our red
distribution above are: The probability of seeing a sample mean of around 52.75
given a population mean of the corresponding x value.</p>

<p>We want to reverse this probability.  We want the probability that the
population mean is a certain value (on the x axis), given that we have sample
mean of around 58.75.</p>

<p>To do this, we follow the rules in <a href="/cfd2019/chapters/10/first_bayes">reverse probability</a> and <a href="/cfd2019/chapters/10/bayes_bars">Bayes bars</a>.</p>

<p>These are:</p>

<ol>
  <li>Get the probabilities of the sample mean given each population mean; these
are the red values in the plot above.  Call these the <em>forward</em>
probabilities.</li>
  <li>Scale the forward probabilities by the initial or <em>prior</em> probability of
each population mean (we will have to decide what those are).</li>
  <li>Divide the results by the sum of the results from step 2 to get the reverse
or <em>posterior</em> probabilities.</li>
</ol>

<p>For step 2, we will assume that there is an equal prior (initial) probability
for each of our possible population means.</p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># All the population means we have tried have the same initial probability.
</span><span class="n">prior_pop_mean_ps</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n_means</span><span class="p">)</span> <span class="o">/</span> <span class="n">n_means</span>
<span class="n">prior_pop_mean_ps</span>
</code></pre></div></div>

<div class="output_data_text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>array([0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02,
       0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02,
       0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02,
       0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02,
       0.02, 0.02, 0.02, 0.02, 0.02, 0.02])
</code></pre></div></div>

<p>Continuing with step 2, we scale the forward probabilities by the prior
probabilities, and divide by the resulting sum, to get the posterior (reverse)
probabilities:</p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">prior_times_forwards</span> <span class="o">=</span> <span class="n">prior_pop_mean_ps</span> <span class="o">*</span> <span class="n">ps_for_mean</span>
<span class="n">posterior_ps</span> <span class="o">=</span> <span class="n">prior_times_forwards</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">prior_times_forwards</span><span class="p">)</span>
<span class="n">posterior_ps</span>
</code></pre></div></div>

<div class="output_data_text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>array([0.0001, 0.0001, 0.0002, 0.0004, 0.0006, 0.0011, 0.0013, 0.0019,
       0.0027, 0.0042, 0.0054, 0.0072, 0.0096, 0.012 , 0.0159, 0.0182,
       0.0229, 0.0268, 0.0319, 0.0353, 0.0388, 0.043 , 0.0465, 0.0495,
       0.0518, 0.0519, 0.0526, 0.0529, 0.0494, 0.0495, 0.0449, 0.0411,
       0.0381, 0.0324, 0.0303, 0.0264, 0.0216, 0.0189, 0.0143, 0.0117,
       0.0096, 0.0073, 0.0059, 0.0041, 0.0033, 0.0022, 0.0015, 0.0013,
       0.0008, 0.0006])
</code></pre></div></div>

<p>These are now the probabilities of each population mean, given the sample mean
of around 58.75.</p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">population_means</span><span class="p">,</span> <span class="n">posterior_ps</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'darkred'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">'P of given population mean, given sample mean'</span><span class="p">);</span>
</code></pre></div></div>

<p><img src="../../images/chapters/10/bayes_confidence_69_0.png" alt="png" /></p>

<p>You might notice that this looks very much like the original plot (in red,
above) of the probabilities of the sample mean, given the population means.</p>

<p>In fact is it is the same, and this is because:</p>

<ul>
  <li>As you saw in <a href="/cfd2019/chapters/10/first_bayes">reverse probability</a> and <a href="/cfd2019/chapters/10/bayes_bars">Bayes bars</a>, when all the prior (initial) probabilities are
the same, we can skip the step of multiplying the forward (red) values by the
prior (initial) probabilities.</li>
  <li>The forward (red) values were proportions, and so all the red values add up
to 1, corresponding to all the sample means.  Therefore, step 3, dividing by
the sum, is dividing by 1, and doesn’t change the values.</li>
</ul>

<p>The dark red distribution is very useful, because it can answer questions we are interested in.</p>

<p>Remember that each value in this plot is the probability of the corresponding
population means (on the x axis), given the observed sample mean of around
58.75 (in fact the sample mean was 58.74, but we will ignore that small
difference for now).</p>

<p>Remember too that the “population” we are interested in here is the eventual
7300 or so marks from 2019.</p>

<p>We see quickly that our sample mean makes it perfectly plausible that the
eventual population mean will be at or above the previous mean of 62 or so,
because a substantial proportion the area of the distribution corresponds to
values of 62 or greater.</p>

<p>In fact, we could do better than this, and work out the population mean
threshold, such that about 5% of the distribution is above threshold. Call this
threshold <code class="highlighter-rouge">t</code>. Once we have found this value, <code class="highlighter-rouge">t</code>, we can say that there is
about a 95% chance that the eventual population mean will be less than or equal
to <code class="highlighter-rouge">t</code>.  We could call <code class="highlighter-rouge">t</code> the 5% upper <em>confidence limit</em>, because we are 95%
confident that the eventual population mean will be less than or equal to this
value.</p>

<p>To get this value, we can use the <code class="highlighter-rouge">np.cumsum</code> function, first mentioned in the
<a href="/cfd2019/chapters/03/Arrays">arrays</a> page. It takes an input array
say <code class="highlighter-rouge">a</code>, and returns another array, say <code class="highlighter-rouge">b</code> that is the same size as <code class="highlighter-rouge">a</code>, but
which has, at each element, the result of summing up all the elements in <code class="highlighter-rouge">a</code> up
to the corresponding position.  This might be clearer by example:</p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># np.cumsum - for each element, add all elements so far.
</span><span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
<span class="n">np</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
</code></pre></div></div>

<div class="output_data_text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>array([ 1,  3,  8, 10, 10])
</code></pre></div></div>

<p>We can do the same with the posterior probabilities:</p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">cumulative_post_ps</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">posterior_ps</span><span class="p">)</span>
<span class="n">cumulative_post_ps</span>
</code></pre></div></div>

<div class="output_data_text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>array([0.0001, 0.0002, 0.0004, 0.0009, 0.0015, 0.0025, 0.0038, 0.0057,
       0.0085, 0.0127, 0.0181, 0.0253, 0.035 , 0.047 , 0.0629, 0.0811,
       0.1039, 0.1307, 0.1626, 0.1979, 0.2367, 0.2797, 0.3261, 0.3756,
       0.4274, 0.4794, 0.532 , 0.5849, 0.6344, 0.6838, 0.7287, 0.7698,
       0.8079, 0.8403, 0.8706, 0.897 , 0.9186, 0.9375, 0.9518, 0.9635,
       0.9731, 0.9804, 0.9862, 0.9903, 0.9936, 0.9958, 0.9973, 0.9986,
       0.9994, 1.    ])
</code></pre></div></div>

<p>Notice that, towards the end of this array, we reach 0.95; this means that the
probability values up to this point add up to 0.95 or more; equivalently, that
we already have 95% of the probability or more at this point.</p>

<p>In the following code, we get True if we have not yet reached 0.95, and False
after that.</p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">cum_ps_lte_0p95</span> <span class="o">=</span> <span class="n">cumulative_post_ps</span> <span class="o">&lt;=</span> <span class="mf">0.95</span>
<span class="n">cum_ps_lte_0p95</span>
</code></pre></div></div>

<div class="output_data_text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>array([ True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True, False, False, False, False, False, False, False,
       False, False, False, False, False])
</code></pre></div></div>

<p>We want the largest of the corresponding population mean values, to get our
<code class="highlighter-rouge">t</code>.</p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">t</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="nb">max</span><span class="p">(</span><span class="n">population_means</span><span class="p">[</span><span class="n">cum_ps_lte_0p95</span><span class="p">])</span>
<span class="n">t</span>
</code></pre></div></div>

<div class="output_data_text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>61.25
</code></pre></div></div>

<p>This calculation tells us that, <em>given our assumptions</em>, there is a 95% chance
that the eventual population mean will be less than or equal to our found <code class="highlighter-rouge">t</code>.
<code class="highlighter-rouge">t</code> is our 95% upper confidence limit.</p>

<p>It looks like we are moderately confident that the mean of the marks will be
less in 2019 than it was in 2018.</p>

<h2 id="a-short-cut-for-the-calculations">A short cut for the calculations</h2>

<p>In this exposition, we started by breaking up the sampling distribution into a
counts values from a histogram; converting the counts into a proportions, and
then transforming the proportions by shifting on the x axis, and reversing
them.  We found this was our posterior probability distribution, and then we
used the cumulative sum of this distribution to find our upper tail threshold.</p>

<p>In fact we can do equivalent operations without having to break up the
distribution into counts, by applying the same transformations to the values
in the sampling distribution.</p>

<p>Here is the sampling distribution (again):</p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">sample_means</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="n">bin_edges</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'Mean mark for sample of 50'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Estimated sampling distribution for mean of 50 marks'</span><span class="p">);</span>
</code></pre></div></div>

<p><img src="../../images/chapters/10/bayes_confidence_80_0.png" alt="png" /></p>

<p>As we did for the proportions, we will shift this distribution to have a mean
at the observed sample mean, and take the mirror image around this point, like
this:</p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Shift distribution to observed sample mean; take mirror image.
# Move distribution mean point to zero.
</span><span class="n">zero_centered</span> <span class="o">=</span> <span class="n">sample_means</span> <span class="o">-</span> <span class="mf">62.25</span>
<span class="c1"># Mirror image around zero point.
</span><span class="nb">reversed</span> <span class="o">=</span> <span class="o">-</span><span class="n">zero_centered</span>
<span class="c1"># Add back the observed sample mean.
</span><span class="n">reversed_shifted</span> <span class="o">=</span> <span class="nb">reversed</span> <span class="o">+</span> <span class="n">observed_sample_mean</span>
<span class="c1"># Show a histogram (and store the counts).
</span><span class="n">rs_counts</span><span class="p">,</span> <span class="n">rs_edges</span><span class="p">,</span> <span class="n">patches</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">reversed_shifted</span><span class="p">,</span>
    <span class="n">bins</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mf">42.5</span><span class="p">,</span> <span class="mf">67.75</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">),</span>
    <span class="n">color</span><span class="o">=</span><span class="s">'darkgreen'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Reversed, shifted sampling distribution'</span><span class="p">);</span>
</code></pre></div></div>

<p><img src="../../images/chapters/10/bayes_confidence_82_0.png" alt="png" /></p>

<p>Here’s the same histogram, but built using the counts we got back from the
call, just to show it is the same.</p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">rs_centers</span> <span class="o">=</span> <span class="n">rs_edges</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="mf">0.25</span>
<span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">rs_centers</span><span class="p">,</span> <span class="n">rs_counts</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'darkgreen'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Reversed, shifted sampling distribution using counts'</span><span class="p">);</span>
</code></pre></div></div>

<p><img src="../../images/chapters/10/bayes_confidence_84_0.png" alt="png" /></p>

<p>Notice that, if we divide the counts in this histogram by the number of
samples, to get proportions, this looks very, very similar to the posterior
probabilities we have just calculated.  The plot below shows the populations
superimposed; they overlap so completely that you cannot distinguish the two.</p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">rs_centers</span><span class="p">,</span> <span class="n">rs_counts</span> <span class="o">/</span> <span class="n">n_samples</span><span class="p">,</span>
        <span class="n">color</span><span class="o">=</span><span class="s">'darkblue'</span><span class="p">,</span>
        <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
        <span class="n">label</span><span class="o">=</span><span class="s">'Reversed, shifted p'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">population_means</span><span class="p">,</span> <span class="n">posterior_ps</span><span class="p">,</span>
        <span class="n">color</span><span class="o">=</span><span class="s">'darkred'</span><span class="p">,</span>
        <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
        <span class="n">label</span><span class="o">=</span><span class="s">'Posterior p'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Reversed shifted and posterior p'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s">'lower left'</span><span class="p">);</span>
</code></pre></div></div>

<p><img src="../../images/chapters/10/bayes_confidence_86_0.png" alt="png" /></p>

<p>Using this reversed shifted version, we can get our threshold rather easily, by
asking for the percentiles of the distribution.</p>

<div class="language-python input_area highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">rs_t</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">percentile</span><span class="p">(</span><span class="n">reversed_shifted</span><span class="p">,</span> <span class="mi">95</span><span class="p">)</span>
<span class="n">rs_t</span>
</code></pre></div></div>

<div class="output_data_text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>61.97
</code></pre></div></div>

<p>This is the 95% percentile, so it is the value such that 95% of the values are
less than this value, and 5% are greater than this value.</p>

<p>The reversed-shifted calculation is very similar to something called the
<em>Reverse Percentile Interval</em> for the <em>bootstrap</em> (see below, and <a href="https://en.wikipedia.org/wiki/Bootstrapping_%28statistics%29#Deriving_confidence_intervals_from_the_bootstrap_distribution">this
Wikipedia
section</a></p>

<p>As you see, the result is near identical to the approximation we used above
when we broke the distribution into counts and used this for the posterior p
calculation.</p>

<h2 id="estimating-the-sampling-distribution">Estimating the sampling distribution</h2>

<p>We emphasized <em>given our assumptions</em>.  One big assumption that we made was
that the sampling distribution of the mean was the same as that for 2018.  That
seems like a strong assumption; the sampling distribution of the mean will
depend on the distribution of the values, and that may have changed in 2019.</p>

<p>To be more general, we might also want to deal with a situation where we do not
have a roughly equivalent population to help us.  Here we had the distribution
of marks from 2018, but we will often be in the situation where we have a
sample mean, and no population to compare against.  What can we do to estimate
the sampling distribution, if all we have is the sample — in our case, the
sample of 50 marks from 2019?</p>

<p>Enter the <a href="/cfd2019/chapters/10/bootstrap">bootstrap</a>.</p>

          
        </section>

        <footer class="page__meta">
          
          


        </footer>

        

        
  <nav class="pagination">
    
      <a href="/cfd2019/chapters/10/equal_initial_p" class="pagination--pager" title="Equal initial P algebra
">Previous</a>
    
    
      <a href="/cfd2019/chapters/end/end_of_beginning" class="pagination--pager" title="
  The end of the beginning

">Next</a>
    
  </nav>


      </div>

      
    </article>
  </div>
</div>

    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><input type="text" id="search" class="search-input" tabindex="-1" placeholder="Enter your search term..." />
    <div id="results" class="results"></div></div>
      </div>
    

    

    
  <script src="/cfd2019/assets/js/main.min.js"></script>
  <script src="https://use.fontawesome.com/releases/v5.0.12/js/all.js"></script>




<script src="/cfd2019/assets/js/lunr/lunr.min.js"></script>
<script src="/cfd2019/assets/js/lunr/lunr-store.js"></script>
<script src="/cfd2019/assets/js/lunr/lunr-en.js"></script>




    <!-- Custom scripts to load after site JS is loaded -->

    <!-- Custom HTML used for the textbooks -->
<!-- Configure, then load MathJax -->
<script type="text/javascript">
  window.MathJax = {
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true,
      processEnvironments: true
    }
  };
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML-full,Safe" type="text/javascript"></script>


<script type="text/javascript">
// --- To auto-embed hub URLs in interact links if given in a RESTful fashion ---
function getJsonFromUrl(url) {
  var query = url.split('?');
  if (query.length < 2) {
    // No queries so just return false
    return false;
  }
  query = query[1];
  // Collect REST params into a dictionary
  var result = {};
  query.split("&").forEach(function(part) {
    var item = part.split("=");
    result[item[0]] = decodeURIComponent(item[1]);
  });
  return result;
}

// Parse a Binder URL, converting it to the string needed for JupyterHub
function binder2Jupyterhub(url) {
  newUrl = {};
  parts = url.split('v2/gh/')[1];
  // Grab the base repo information
  repoinfo = parts.split('?')[0];
  var [org, repo, ref] = repoinfo.split('/');
  newUrl['repo'] = ['https://github.com', org, repo].join('/');
  newUrl['branch'] = ref
  // Grab extra parameters passed
  params = getJsonFromUrl(url);
  if (params['filepath'] !== undefined) {
    newUrl['subPath'] = params['filepath']
  }
  return jQuery.param(newUrl);
}

// Filter out potentially unsafe characters to prevent xss
function safeUrl(url)
{
   return String(encodeURIComponent(url))
            .replace(/&/g, '&amp;')
            .replace(/"/g, '&quot;')
            .replace(/'/g, '&#39;')
            .replace(/</g, '&lt;')
            .replace(/>/g, '&gt;');
}

function addParamToInternalLinks(hub) {
  var links = $("a").each(function() {
    var href = this.href;
    // If the link is an internal link...
    if (href.search("https://matthew-brett.github.io") !== -1 || href.startsWith('/') || href.search("127.0.0.1:") !== -1) {
      // Assume we're an internal link, add the hub param to it
      var params = getJsonFromUrl(href);
      if (params !== false) {
        // We have REST params, so append a new one
        params['hub'] = hub;
      } else {
        // Create the REST params
        params = {'hub': hub};
      }
      // Update the link
      var newHref = href.split('?')[0] + '?' + jQuery.param(params);
      this.setAttribute('href', decodeURIComponent(newHref));
    }
  });
  return false;
}

  // Update interact links
function updateInteractLink() {
    // hack to make this work since it expects a ? in the URL
    rest = getJsonFromUrl("?" + location.search.substr(1));
    hubUrl = rest['hub'];
    if (hubUrl !== undefined) {
      // Sanitize the hubUrl
      hubUrl = safeUrl(hubUrl);
      // Add HTTP text if omitted
      if (hubUrl.indexOf('http') < 0) {hubUrl = 'http://' + hubUrl;}
      link = $("a.interact-button")[0];
      if (link !== undefined) {
          // Update the interact link URL
          var href = link.getAttribute('href');
          if ('binder' == 'binder') {
            // If binder links exist, we need to re-work them for jupyterhub
            first = [hubUrl, 'hub', 'user-redirect', 'git-sync'].join('/')
            href = first + '?' + binder2Jupyterhub(href);
          } else {
            // If JupyterHub links, we only need to replace the hub url
            href = href.replace("https://mybinder.org", hubUrl);
          }
          link.setAttribute('href', decodeURIComponent(href));

          // Add text after interact link saying where we're launching
          hubUrlNoHttp = decodeURIComponent(hubUrl).replace('http://', '').replace('https://', '');
          $("a.interact-button").after($('<div class="interact-context">on ' + hubUrlNoHttp + '</div>'));

      }
      // Update internal links so we retain the hub url
      addParamToInternalLinks(hubUrl);
    }
}

// --- Highlight the part of sidebar for current page ---

// helper to replace trailing slash
function replaceSlash(string)
{
    return string.replace(/\/$/, "");
}

// Add a class to the current page in the sidebar
function highlightSidebarCurrentPage()
{
  var currentpage = location.href;
  var links = $('.sidebar .nav__items a');
  var ii = 0;
  for(ii; ii < links.length; ii++) {
    var link = links[ii];
    if(replaceSlash(link.href) == replaceSlash(currentpage)) {
      // Add CSS for styling
      link.classList.add("current");
      // Scroll to this element
      $('div.sidebar').scrollTop(link.offsetTop - 300);
    }
  }
}

// --- Set up copy/paste for code blocks ---
function addCopyButtonToCode(){
  // get all <code> elements
  var allCodeBlocksElements = $( "div.input_area code, div.highlighter-rouge code" );

  allCodeBlocksElements.each(function(ii) {
   	// add different id for each code block

  	// target
    var currentId = "codeblock" + (ii + 1);
    $(this).attr('id', currentId);

    //trigger
    var clipButton = '<button class="btn copybtn" data-clipboard-target="#' + currentId + '"><img src="https://clipboardjs.com/assets/images/clippy.svg" width="13" alt="Copy to clipboard"></button>';
       $(this).after(clipButton);
    });

    new Clipboard('.btn');
}

// Run scripts when page is loaded
$(document).ready(function () {
  // Add anchors to H1 etc links
  anchors.add();
  // Highlight current page in sidebar
  highlightSidebarCurrentPage();
  // Add copy button to code blocks
  addCopyButtonToCode();
  // Update the Interact link if a REST param given
  updateInteractLink();
});
</script>

  </body>
</html>
